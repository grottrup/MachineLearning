{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS| |\n",
    "---------| |\n",
    "2018-0301| CEF, initial.\n",
    "2018-0306| CEF, updated.\n",
    "2018-0307| CEF, split Qb into Qb+c+d and added NN comment.\n",
    "2018-0311| CEF, updated Qa and $w_0$ issues.\n",
    "2018-0311| CEF, updated Qd with plot and Q.\n",
    "2018-0311| CEF, clarified $w_0$ issue and update $\\tilde{J}$'s.\n",
    "\n",
    "\n",
    "## Regulizers\n",
    "\n",
    "### Resume of The Linear Regressor\n",
    "\n",
    "For our  data set $\\mathbf{X}$ and target $\\mathbf{y}$ \n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\st[1]{_{\\mbox{\\scriptsize #1}}}\n",
    "    \\newcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\newcommand\\obs[2]{#1_{\\mbox{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\newcommand\\diff[1]{\\mbox{d}#1}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\mbox{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\mbox{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\newcommand\\pfrac[2]{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\newcommand\\dfrac[2]{\\frac{\\mbox{d}~#1}{\\mbox{d}#2}}\n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "            \\vdots      &             &        & \\vdots \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "    }\n",
    ", ~~~~~~~~\n",
    "\\by =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1} \\\\\n",
    "     y\\pown{2} \\\\\n",
    "     \\vdots \\\\\n",
    "     y\\pown{n} \\\\\n",
    "  }\n",
    "$$\n",
    "\n",
    "a __linear regressor__  model was previously found to be of the form\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\mbox{MSE}(\\bX,\\by;\\bw) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                            &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni \\right)^2\\\\\n",
    "                            &\\propto ||\\bX \\bw - \\by||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "here using the squared Euclidean norm, $\\norm{2}^2$, via the $||\\cdot||_2^2$ expressions. We used the MSE to express the total cost function, $J$, as\n",
    "\n",
    "$$\n",
    "   \\mbox{MSE} \\propto J = ||\\bX \\bw - \\by||_2^2\n",
    "$$\n",
    "\n",
    "give or take a few constants, like $1/2$ or $1/n$.\n",
    "\n",
    "### Adding Regularization to the Linear Regressor\n",
    "\n",
    "Now the weights, $\\bw$, in this model are free to take on any value they like, and this can, among other things lead to numerical problems, if the algorithms decide to drive the weights to insane, humongous values, say $10^{200}$ or similar.\n",
    "\n",
    "Also for some models, neural networks, in particular, having weights outside the range -1 to 1 (or 0 to 1) may cause complete saturation of some of the internal non-linear components (the activation function)...we will revisit this problem when we reach neural networks. \n",
    "\n",
    "Now, enters the regularization of the model: keep the weights at a sane level while doing the numerical GD in the search space. This can quite simply be done by adding a __penalty__ part, $\\Omega$, to the $J$ function as\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        \\tilde{J} &= J + \\alpha \\Omega(\\bw)\\\\\n",
    "                  &= \\frac{1}{n} ||\\bX \\bw - \\by||_2^2 + \\alpha ||\\bw||^2_2\n",
    "     }\n",
    "$$\n",
    "\n",
    "So, the algorithm now has to find an optimal value (minimum of $J$) for both the usual MSE part and for the added penalty scaled with the $\\alpha$ constant.\n",
    "\n",
    "### Regularization and Optimization for Neural Networks (NNs)\n",
    "\n",
    "The regularization method mentioned here is strictly for a linear regression model, but such a model constitutes a major part of a neuron, used in neural networks. It is hence important to get a grasp on regularization and optimization, before we start to dig onto NN...\n",
    "\n",
    "### Qa The Penalty Factor\n",
    "\n",
    "Now, lets examine  what $||\\bw||^2_2$ effectivly mean? It is composed of our well-known $\\norm{2}^2$ norm and can also be expressed as\n",
    "\n",
    "$$\n",
    "  ||\\bw||^2_2 = \\bw^\\top\\bw\n",
    "$$\n",
    "\n",
    "Construct a penaltiy function that implements $\\bw^\\top\\bw$, re-using any functions from `numpy` (implementation could be a tiny _one-liner_).\n",
    "\n",
    "Take $w_0$ into account, this weight factor should NOT be included in the norm. Also checkup on `numpy`s `dot` implementation, if you have not done so and are using it: it is a typical pythonic _combo_ function, doing both dot op's (inner product) and matrix multiplication (outer product) dependent on the shape of the input parameters.\n",
    "\n",
    "Then run it on the three test vectors below, and explain when the penalty factor is low and when it is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa\n",
    "\n",
    "import numpy as np\n",
    "from libitmal import utils as itmalutils\n",
    "\n",
    "def Omega(w):\n",
    "    assert False, \"TODO: add impl. here and remove this assert\"\n",
    "    \n",
    "# weight vector format: [w_0 w_1 .. w_d], ie. elem. 0 is the 'bias'    \n",
    "w_a = np.array([1, 2, -3]) # \n",
    "w_b = np.array([1E10, -3E10])\n",
    "w_c = np.array([0.1, 0.2, -0.3, 0])\n",
    "\n",
    "p_a = Omega(w_a)\n",
    "p_b = Omega(w_b)\n",
    "p_c = Omega(w_c)\n",
    "\n",
    "print(f\"P(w0)={p_a}\")\n",
    "print(f\"P(w1)={p_b}\")\n",
    "print(f\"P(w2)={p_c}\")\n",
    "\n",
    "# TEST VECTORS\n",
    "e0 = 2*2+(-3)*(-3)\n",
    "e1 = 9e+20\n",
    "e2 = 0.13\n",
    "itmalutils.AssertInRange(1.0*p_a,1.0*e0)\n",
    "itmalutils.AssertInRange(1.0*p_b,1.0*e1,eps=1E5)\n",
    "itmalutils.AssertInRange(1.0*p_c,1.0*e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Regularization for Linear Regression Models\n",
    "\n",
    "Adding the penalty $\\alpha ||\\bw||^2_2$ actually corresponds to the Scikit-learn model `sklearn.linear_model.Ridge` and there are, as usual, a bewildering array of regulized models to choose from in Scikit-learn with exotic names like `Lasso` and `Lars`\n",
    "\n",
    "> https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n",
    "\n",
    "Lets just examine `Ridge`, `Lasso` and `ElasticNet` here.\n",
    "\n",
    "### Qb Explain the Ridge Plot\n",
    "\n",
    "First take a peek into the plots (and code) below, that fits the `Ridge`, `Lasso` and `ElasticNet` to a polynomial model. The plots show three fits with different $\\alpha$ values (0, 10$^{-5}$, and 1).\n",
    "\n",
    "First, explain what the different $\\alpha$ does to the actual fitting for the `Ridge` model in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb, just run the code..\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, ElasticNet, Lasso\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def FitAndPlotModel(name, model_class, X, X_new, y, **model_kargs):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    alphas=(0, 10**-5, 1) \n",
    "    random_state=42\n",
    "    \n",
    "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
    "        #print(model_kargs)\n",
    "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
    "        model_pipe = Pipeline([\n",
    "                (\"poly_features\", PolynomialFeatures(degree=12, include_bias=False)),\n",
    "                (\"std_scaler\", StandardScaler()),\n",
    "                (\"regul_reg\", model),\n",
    "            ])\n",
    "            \n",
    "        model_pipe.fit(X, y)\n",
    "        y_new_regul = model_pipe.predict(X_new)\n",
    "        \n",
    "        lw = 2 if alpha > 0 else 1\n",
    "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
    "    \n",
    "    plt.plot(X, y, \"b.\", linewidth=3)\n",
    "    plt.legend(loc=\"upper left\", fontsize=15)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.title(name)\n",
    "    plt.axis([0, 3, 0, 4])\n",
    "\n",
    "def GenerateData():\n",
    "    np.random.seed(42)\n",
    "    m = 20\n",
    "    X = 3 * np.random.rand(m, 1)\n",
    "    y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
    "    X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
    "    return X, X_new, y\n",
    "    \n",
    "X, X_new, y = GenerateData()\n",
    "\n",
    "FitAndPlotModel('ridge',      Ridge,        X, X_new, y)\n",
    "FitAndPlotModel('lasso',      Lasso,        X, X_new, y)\n",
    "FitAndPlotModel('elasticnet', ElasticNet,   X, X_new, y, l1_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Explain the Ridge, Lasso and ElasticNet Regulized Methods\n",
    "\n",
    "Then explain the different regularization methods used for the `Ridge`, `Lasso` and `ElasticNet` models, by looking at the math formulas for the methods in the Scikit-learn documentation and/or using [HOML]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc, in text (no code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Regularization and Overfitting\n",
    "\n",
    "Finally, comment on how regularization may be used to reduce a potential tendency to overfit the data\n",
    "\n",
    "Describe the situation with the tug-of-war between the MSE and regulizer terms in $\\tilde{J}$ and the potential problem of $\\bw^*$ being far, far away from the origin (with $\\alpha=1$ in regulizer term).\n",
    "\n",
    "<img src=\"Figs/weights_regularization.png\" style=\"width:240px\">\n",
    "\n",
    "Would data preprocessing in the form of scaling, standardization or normalization be of any help to that particular situation? If so, describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd, in text (no code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
