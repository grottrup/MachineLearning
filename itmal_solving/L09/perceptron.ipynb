{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS| |\n",
    "---------| |\n",
    "2018-0318| CEF, initial.\n",
    "2018-0321| CEF, updated and split into neurons and perceptrons.\n",
    "2018-0323| CEF, minor updated and spell checked.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "Ok, thanks to biology for giving us a model of a neuron to work with. The neuron, or perceptron, is in ML given by a modified version of the linear regressor. Let us visit the mathematical model for the artificial neuron using _Rosenblatt's perceptron_ or _linear threshold unit (LTU)_. \n",
    "\n",
    "First, we have the usual $d$-dimension input feature vector, $\\mathbf{x}$, and some model weights, $\\mathbf{w}$, that multiplied together gave us the linear regression model output, this time dubbed $z$\n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\st[1]{_{\\mbox{\\scriptsize #1}}}\n",
    "    \\newcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\newcommand\\obs[2]{#1_{\\mbox{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\newcommand\\diff[1]{\\mbox{d}#1}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\mbox{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\mbox{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\newcommand\\pfrac[2]{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\newcommand\\dfrac[2]{\\frac{\\mbox{d}~#1}{\\mbox{d}#2}}    \n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\\ar{rl}{\n",
    "    z\\powni &= \\bw^\\top \\ac{c}{1\\\\\\bx\\powni} \\\\\n",
    "            &= w_0 + w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni\n",
    "}\n",
    "$$\n",
    "\n",
    "remembering to add the bias weight, $w_0$, either explicitly, as $z = w_0 + \\bw^\\top\\bx $ or by redefining our $\\bx$ to have an extra 1 prepended (as usually done in linear regression)\n",
    "\n",
    "$$\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} &\\mapsto \\bx\\powni, ~~~~\\mbox{by convention in the following...}\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "So, to ease notation, we just write  \n",
    "\n",
    "$$\n",
    "    z\\powni = \\bw^\\top \\bx\\powni \n",
    "$$\n",
    "\n",
    "in the following assuming that $\\bx$ also contains a bias element, with value 1.\n",
    "\n",
    "To make a neuron, we just extend the linear regression with a _non-linear activation_ function, $a(\\cdot)$, that takes $z$ as input\n",
    "\n",
    "$$\n",
    "    y_{neuron}\\powni(\\bx\\powni;\\bw) = a(z) = a(\\bw^\\top\\bx\\powni)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<img src=\"Figs/perceptron.png\" style=\"width:250px\">\n",
    "\n",
    "That's it, a model nearly similar to the linear regression, but now with the added non-linear activation function. Lets put an artificial neuron, a perceptron, into action...\n",
    "\n",
    "NOTE: [HOML] uses the term _perceptron_ to be a single __layer__ network of neurons, not just a single Rosenblatt's perceptron.\n",
    "\n",
    "### Qa Using a Perceptron on the Moon-data\n",
    "\n",
    "Load the moon data, split it into a train-and-test set, and train a `sklearn.linear_model.Perceptron` with default parameters (and with no cross-val this time).\n",
    "\n",
    "What is the default score metric for the perceptron and what score do reach on the test data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa..\n",
    "\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from libitmal import dataloaders_v3 as itmaldataloaders\n",
    "\n",
    "X, y = itmaldataloaders.MOON_GetDataSet(n_samples=1000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "assert False, \"TODO: add perceptron hyperparams, train it and get the default score metric on the test data\"\n",
    "#model = Perceptron(.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb: Plot the Decision Boundary\n",
    "\n",
    "Use the helper code below to plot the decision boundary for the perceptron on the moon-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb..helper code for plotting decision boundaries\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function to plot a decision boundary.\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral,alpha=.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, alpha=.5)\n",
    "\n",
    "# Predict and plot decision boundary\n",
    "assert False, \"insert you perceptron 'model' here...and uncomment this line..\"\n",
    "plot_decision_boundary(lambda x: model.predict(x))\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc The Perceptron on an XOR-problem\n",
    "\n",
    "How does the moon data relate to the well-known _XOR_-problem in machine learning?\n",
    "\n",
    "Is it possible for a single neuron or perceptron to solve the moon/XOR problem satisfactorily?\n",
    "\n",
    "Elaborate on the reason why not, and give a comment on how to overcome the XOR-problematic using more perceptrons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc..in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Compare the Perceptron to the SGD\n",
    "\n",
    "The Perceptron is said to be similar to the Stochastic Gradient Descent classifier.\n",
    "\n",
    "Try out the SGD, with a parameter setup compatible with the perceptron, see the _notes_  section\n",
    "\n",
    "> _`Perceptron` is a classification algorithm which shares the same underlying implementation with `SGDClassifier`. In fact, `Perceptron()` is equivalent to_ <br><br>\n",
    "`SGDClassifier(loss='perceptron', eta0=1, learning_rate='constant', penalty=None)` <br><br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "\n",
    "On the same train-test data train an SGD with perceptron compatible parameters. Give both models a `random_seed=42` and `tol=1e-3` parameters. \n",
    "\n",
    "Are the SGD and Perceptron score metrics also compatible?\n",
    "\n",
    "Does the SGD yield the same score as the Perceptron? \n",
    "\n",
    "(100% similar scores, nearly similar or not at all similar?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
