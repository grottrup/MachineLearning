{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS| |\n",
    "---------| |\n",
    "2018-0318| CEF, initial.\n",
    "2018-0321| CEF, finally got moon fit to work, it is rather prone to flat-plateau gradient problems.\n",
    "2018-0321| CEF, major revision.\n",
    "2018-0323| CEF, minor updated and spell checked.\n",
    "\n",
    "## Multi-layers Perceptrons (MLP)\n",
    "\n",
    "By stacking up neurons in layers we can create an MLP.\n",
    "\n",
    "<img src=\"Figs/mlpfully_simple.png\" style=\"width:300px\">\n",
    "<small><em>\n",
    "    <center>An example of a fully-connected three-layer MLP.</center> \n",
    "</xsmall></center>\n",
    "</em></small>\n",
    "\n",
    "\n",
    "## Keras Multi-Layer Perceptrons: the `Sequential` model\n",
    "\n",
    "In this exercise well will try to build fully-connected MLP, via the `keras` API.  Once Keras is installed (good luck), you can use it as front-end to Tensorflow and its highly optimized MLP implementations.\n",
    "\n",
    "In Keras-terms an MLP is a `Sequential` model. This model can be made out of fully-connected, or in Keras terms `Dense`, layers. Later we see other types of layers that are not fully-connected, like convolutional 2D layers, Conv2D.\n",
    "\n",
    "OPTIONAL: more info on Keras' `Sequential` model\n",
    "\n",
    "> https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "## Keras Multi-Layer Perceptrons on Moon-data\n",
    "\n",
    "Below are some cells for running a Keras `Sequential` model on the moon-data.\n",
    "\n",
    "The first cell setup the keras model, load the data and initiates the fit.\n",
    "\n",
    "The second and third cell plots various aspects of the training, using the _history_ from the Keras model.\n",
    "\n",
    "Now, many Keras functions are not as well documented compared to Scikit-learn and even though many elements seem to be equal in the Keras and Sckikit-learn domain, they are not 100% equal, for example, Keras comes with some metrics like\n",
    "\n",
    "> `categorical_accuracy` and `binary_accuracy`\n",
    "\n",
    "but has no `F1` or `precision` or `recall`.\n",
    "\n",
    "### Qa Using a Keras MLP on the Moon-data\n",
    "\n",
    "Run the three cells below, and inspect the plots. I get an accuracy of 0.96 using the setup below.\n",
    "\n",
    "Now, change the optimizer from `Adam` to our well-known `SDG` method, using\n",
    "\n",
    "> `optimizer = SGD(lr=0.1)`\n",
    "\n",
    "instead of `ADAM(lr=0.1)`.\n",
    "\n",
    "Does it still produce a good score, in form of the `categorical_accuracy`? My accuracy now drops to 0.88, and the new decision boundary looks like a straight line!\n",
    "\n",
    "Find a way to make the `SDG` produce a result similar to the `ADAM` optimizer: Maybe you need to crack up the number of `EPOCHS` during training to get a better result using the `SGD` optimizer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, training time=1.4\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa..run Keras on Moon, cell 1\n",
    "\n",
    "from libitmal import kernelfuns as itmalkernelfuns\n",
    "itmalkernelfuns.EnableGPU()                              \n",
    "#itmalkernelfuns.DisableGPU()   \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build Keras model \n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=2, units=8, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "model.add(Dense(units=2, activation=\"softmax\"))\n",
    "\n",
    "#optimizer = SGD(lr=0.1)\n",
    "optimizer = Adam(lr=0.1)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# Make data\n",
    "X, y = datasets.make_moons(2000, noise=0.20, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train_binary = to_categorical(y_train)\n",
    "y_test_binary  = to_categorical(y_test)\n",
    "\n",
    "assert y.ndim==1\n",
    "assert y_train_binary.ndim==2\n",
    "assert y_test_binary.ndim ==2\n",
    "\n",
    "# Train\n",
    "VERBOSE     = 0\n",
    "EPOCHS      = 35\n",
    "\n",
    "start = time()\n",
    "history = model.fit(X_train, y_train_binary, validation_data=(X_test, y_test_binary), epochs=EPOCHS, verbose=VERBOSE)\n",
    "t = time()-start\n",
    "\n",
    "print(f\"OK, training time={t:0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa..run Keras on Moon, cell 2\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(history.history)\n",
    "score = model.evaluate(X_test, y_test_binary, verbose=0)\n",
    "\n",
    "print(f\"Training time: {t:0.1f} sec\")\n",
    "print(f\"Test loss:     {score[0]}\") # loss is score 0 by definition?\n",
    "print(f\"Test accuracy: {score[1]}\")\n",
    "print(f\"All scores in history: {score}\")\n",
    "\n",
    "N=4\n",
    "FX=60\n",
    "FY=4\n",
    "A=0.4\n",
    "S=4\n",
    "\n",
    "# Plot org data\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 1)\n",
    "colors = ['steelblue' if label == 1 else 'darkred' for label in y]\n",
    "plt.scatter(X[:,0], X[:,1], color=colors, alpha=.5)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 2)\n",
    "plt.plot(history.history[\"loss\"]    , \"b--x\", markerfacecolor=(0, 0, 1, A), markersize=S)\n",
    "plt.plot(history.history[\"val_loss\"], \"g-s\" , markerfacecolor=(0, 1, 0, A), markersize=S)\n",
    "plt.legend(loc=\"best\", labels=(\"loss(train)\",\"loss(val)\"))\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss-vs-epoch plot\")\n",
    "plt.show()\n",
    "\n",
    "# Plot all metrics + loss\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 3)\n",
    "plt.plot(history.history[\"mean_squared_error\"],      \"r:x\", markerfacecolor=(1, 0, 0, A), markersize=S)\n",
    "plt.plot(history.history[\"val_mean_squared_error\"],  \"r-x\", markerfacecolor=(1, 0, 0, A), markersize=S)\n",
    "plt.plot(history.history[\"mean_absolute_error\"],     \"b:o\", markerfacecolor=(0, 0, 1, A), markersize=S)\n",
    "plt.plot(history.history[\"val_mean_absolute_error\"], \"b-o\", markerfacecolor=(0, 0, 1, A), markersize=S)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.xlim((0, EPOCHS))\n",
    "plt.legend(loc=\"best\", labels=(\"mean_squared_error(train)\",  \"mean_squared_error(val)\", \n",
    "                               \"mean_absolute_error(train)\", \"mean_absolute_error(val)\", \n",
    "                               \"loss(categorical_crossentropy,train)\", \"loss(categorical_crossentropy,val)\"))\n",
    "plt.title(\"Error-vs-epoch plot\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 4)\n",
    "plt.plot(history.history[\"categorical_accuracy\"],     \"m-x\", markerfacecolor=(1, 0, 1, A), markersize=S)\n",
    "plt.plot(history.history[\"val_categorical_accuracy\"], \"m:x\", markerfacecolor=(1, 0, 1, A), markersize=S)\n",
    "ax.set_ylim([0,1])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlim((0, EPOCHS))\n",
    "plt.legend(loc=\"lower right\", labels=(\"categorical_accuracy\",))\n",
    "plt.title(\"Accuracy-vs-epoch plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa..run Keras on Moon, cell 3\n",
    "\n",
    "# Helper function to plot a decision boundary.\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral,alpha=.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, alpha=.5)\n",
    "\n",
    "# Predict and plot decision boundary\n",
    "plot_decision_boundary(lambda x: model.predict_classes(x, batch_size=200))\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb  Keras and Classification Categories\n",
    "\n",
    "It is customary practice to convert both binary and multiclass classification labels to a one-hot encoding. \n",
    "\n",
    "Explain one-hot encoding and the \n",
    "\n",
    "```python\n",
    "y_train_binary = to_categorical(y_train)\n",
    "y_test_binary  = to_categorical(y_test)\n",
    "```\n",
    "and the used categorical metric (compare it to our well know accuracy function),\n",
    "\n",
    "```python\n",
    "metrics=['categorical_accuracy',..\n",
    "```\n",
    "\n",
    "NOTE: Keras'  `categorical_accuracy` is implemented as\n",
    "```\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return K.cast(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)), K.floatx())\n",
    "\n",
    "```\n",
    "but also used internal TensorFlow tensors instead of `numpy.arrays` and these are right now difficult to work with directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb..explain in text or create your own categorical_accuracy fun.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Optimize the Keras Model\n",
    "\n",
    "Now, try to optimize the model by \n",
    "\n",
    "* increasing/decreasing the number of epochs,\n",
    "* adding more neurons per layer, \n",
    "* adding whole new layers,\n",
    "* changing the activation functions in the layers,\n",
    "* changing the output activation from `activation=\"softmax\"` to something else,\n",
    "\n",
    "Comment on your changes, and relate the resulting accuracy, accuracy-vs-epochs, loss-vs-epoch and decision boundary plots to your changes, ie. try to get a feeling of what happens when you modify the model hyperparameters. \n",
    "\n",
    "NOTE: Many times the model seems to get stuck on an extreme flat loss plateau, and the decision boundary displays just a 'dum' straight line through the moons!\n",
    "\n",
    "OPTIONAL: should the moon data be standardized or normalized to say range [-1;1] in both $\\mathbf x$-dimensions? Will it help, or is the data OK as-is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Qd Cross Entropy\n",
    "\n",
    "Explain loss='categorical_crossentropy. What is cross-entropy, and how is it used as a norm/distance? Why choose cross-entropy instead of MSE or MAE?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd in text.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
