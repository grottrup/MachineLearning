{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS|\n",
    "---------|-----------\n",
    "2018-1218|CEF, initial.                  \n",
    "2019-0131|CEF, spell checked and update. \n",
    "2019-0131|CEF, updated Pearson's r to work with random vars. instead of matrices.\n",
    "2019-0131|CEF, updated mean-var fun and biased/un-biased var estimator update.\n",
    "2019-0204|CEF, minor Pearsons's r update.\n",
    "2019-0204|CEF, changed headline.\n",
    "2019-0204|CEF, added bias comment in cross- auto-covariance.\n",
    "2019-0204|CEF, changed eqs for cross- auto-covariance.\n",
    "2019-0205|CEF, spell checked. \n",
    "\n",
    "## Statistics\n",
    "\n",
    "### Mean and Variance\n",
    "\n",
    "The mean and variance (and hence the standard deviation) for a random variable $X$ can for a population of $N$ samples be estimated as\n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, rember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\st[1]{_{\\mbox{\\scriptsize #1}}}\n",
    "    \\newcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\newcommand\\obs[2]{#1_{\\mbox{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\newcommand\\diff[1]{\\mbox{d}#1}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\mbox{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\mbox{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\begin{array}{rl}\n",
    "        E[X] &= \\frac{1}{N} \\sum_{i=1}^N X_i\\\\\n",
    "             &= \\mu_X\\\\\n",
    "        V[X] &= E[(X_i -E[X])(X_i -E[X])] \\\\\n",
    "             &= E[X_i X_i] - E[X]^2\\\\\n",
    "             &= \\left( \\frac{1}{N} \\sum_{i=1}^N X_i X_i \\right) - \\mu_X^2\\\\\n",
    "             &= \\sigma_X^2\\\\\n",
    "        \\sigma & = \\sqrt{V}\n",
    "    \\end{array}\n",
    "$$\n",
    "\n",
    "Notice that the $1/N$ factor most often appears as $1/(N-1)$ for the variance estimation. \n",
    "\n",
    "When using the factor $1/(N-1)$, $\\hat V$ is said to be the best unbiased estimator, when it is $1/N$ it will be biased, both assuming an underlying normal distribution. \n",
    "\n",
    "Be very careful when rewriting the biased equation above to the unbiased variance estimator. Anyway, for large $N$ this factor has less effect.\n",
    "\n",
    "#### Qa Create a mean and variance function for some input data\n",
    "\n",
    "Your python function should be named ```MeanAndVariance()```, it takes a vector as input parameter, and returns TWO parameters, namely mean and variance. \n",
    "\n",
    "Python allows for return of zero, one, or more parameters from a function, without having to place, say two output parameters in a tuple, like in C++ ```return make_pair<float,float>(mean, variance)```.\n",
    "\n",
    "Test it via the ```y``` input and test-vectors below, go for the biased variance estimator at first.\n",
    "\n",
    "Then extend it to handle both biased and un-biased estimators, create your own test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa...\n",
    "\n",
    "#def MeanAndVariance(x):    \n",
    "    # Your impl here\n",
    "\n",
    "\n",
    "# TEST vectors: mean and variance calc\n",
    "y = np.array([1,2,3,4])\n",
    "m, v = MeanAndVariance(y)\n",
    "\n",
    "expected_m = 2.5  \n",
    "expected_v_biased = 1.25 # factor 1/n\n",
    "expected_v_unbiased = 1.6666666666666667 # factor 1/(n-1)\n",
    "\n",
    "print(\"m=\",m,\", diff=\", m-expected_m)\n",
    "print(\"v=\",v,\", diff=\", v-expected_v_biased)\n",
    "print(v,np.mean(y), np.var(y)) # np.var is biased(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auto- and Cross-covariance in Matrix Notation\n",
    "\n",
    "Now let's goto full matrix notation for the covariance. For a data matrix $\\bX$  \n",
    "\n",
    "$$\n",
    "    \\bX = \\ac{cccc}{\n",
    "        x_1^{(1)} & x_2^{(1)} & \\cdots & x_d^{(1)} \\\\\n",
    "        x_1^{(2)} & x_2^{(2)} & \\cdots & x_d^{(2)}\\\\\n",
    "        \\vdots \\\\\n",
    "        x_1^{(n)} & x_2^{(n)} & \\cdots & x_d^{(n)}\\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "the previous one-dimensional random variable $X$ can now be seen as a $d$-dimensional vector $\\bx\\powni$, that is one of the data rows in the full data matrix\n",
    "\n",
    "$$\n",
    "    X_i \\to \\bx\\powni = \\left[  x_1\\powni~~ x_2\\powni~~ \\cdots~~ x_d\\powni\\right]^T\n",
    "$$\n",
    "\n",
    "The (biased) autoâ€“covariance matrix can be estimated as\n",
    "\n",
    "$$\n",
    "  \\ar{rl}{\n",
    "            E[\\bX] &= \\mu_\\bX \\\\\n",
    "                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\bx\\powni \\\\ \\\\\n",
    "       \\Sigma(\\bX) &= \\mbox{cov}(\\bX,\\bX) \\\\\n",
    "                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\bx\\powni\\bx^{(i)T} - \\mu_\\bX\\mu_\\bX^T \n",
    "   }\n",
    "   \\rem{\n",
    "     \\ar{ll}{\n",
    "        \\Sigma(\\bX) &= \\mbox{cov}(\\bX,\\bX)\\\\\n",
    "          &= E\\left[(\\bX-\\mu_\\bX)(\\bX-\\mu_\\bX)^T \\right]\\\\\n",
    "          &= E\\left[ \\bX\\bX^T \\right] - E[\\bX]E[\\bX]^T\\\\\n",
    "          &= E\\left[ \\bX\\bX^T \\right] - \\mu_\\bX\\mu_\\bX^T\n",
    "      }\n",
    "   }\n",
    "$$\n",
    "\n",
    "with implicit $1/N$ factor here (inside the first $E[\\cdot]$), so the definition is a biased covariance. You may opt to use the factor $1/(N-1)$ instead.\n",
    "\n",
    "For yet another data matrix $\\bZ $ the (biased) cross-covariance matrix is given by\n",
    "\n",
    "$$\n",
    "  \\ar{ll}{\n",
    "       \\Sigma(\\bX,\\bZ) &= \\mbox{cov}(\\bX,\\bZ)\\\\\n",
    "            &= \\frac{1}{n} \\sum_{i=1}^{n} \\bx\\powni\\bz^{(i)T} - \\mu_\\bX\\mu_\\bZ^T \n",
    "        \\rem{\n",
    "            &= E\\left[(\\bX-\\mu_\\bX)(\\bZ-\\mu_\\bZ)^T \\right]\\\\\n",
    "            &= E\\left[ \\bX\\bZ^T \\right] - E[\\bX]E[\\bZ]^T\\\\\n",
    "            &= E\\left[ \\bX\\bZ^T \\right] - \\mu_\\bX\\mu_\\bZ^T\n",
    "        }\n",
    "   }\n",
    "$$\n",
    "(NOTE: equation to-be confirmed)\n",
    "\n",
    "#### Qb Create a function that generates the auto-covariance matrix $\\Sigma(\\bX)$.\n",
    "\n",
    "Create the function from scratch or find some suitable implementation on the net. \n",
    "\n",
    "Direct use of `numpy.cov` (or other libs) not permitted, though it could serve as a test-stub.  Be very careful regarding the normalization factor when comparing with numpy's or matlabs covar functions. If you are going to use `numpy.cov` for cross-testing, be sure to read up on its `rowvar` parameter!  \n",
    "\n",
    "Make some suitable test data, perhaps share a test-stub with another ITMAL group.\n",
    "\n",
    "Explain the elements in the $\\Sigma$ matrix, what are the diagonal elements $\\Sigma_{ii}$, and what does off-diagonal elements, $\\Sigma_{ij}$ for $i \\neq j$ represent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson's r\n",
    "\n",
    "Now, let us step back again from matrix-vectors to the random variables, $X$ and $Y$. The standard correlation, or Pearson's r, is given by the 'normalized' cross-correlation between the random variables, $X$ and $Y$\n",
    "\n",
    "$$\n",
    "    \\mbox{Pearson's r} = \\rho(X,Y) = \\frac{cov(X,Y)}{\\sigma(X)\\sigma(Y)}\n",
    "$$\n",
    "\n",
    "The (dimensionless) Pearson's r is used in [HOML] and is somewhat similar to the (also dimensionless) $R^2$ score we used last time. \n",
    "\n",
    "Let's try to implement the Pearsons's r now...\n",
    "\n",
    "#### Qc Create a Pearson's r function\n",
    "\n",
    "Again, create some test data, and implement your own function or get inspiration by searching the net for code.\n",
    "\n",
    "You need a full implementation, again not use of libs; so direct use of `numpy.corrcoef` not permitted. But it will serve as a good test function...\n",
    "\n",
    "Write a short description of Pearson's r, giving is range (possible min/max values) and a description of what they mean, and classify it as being a loss/scoring function. \n",
    "\n",
    "Hint: look at these links for the Pearson's r...you can find both algorithm details\n",
    "as well as test data here\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html\n",
    "http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "\n",
    "\n",
    "OPTIONAL: try to re-create some of the data for the plots in figure 2-14 [HOML], and run your Pearson's r function on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
