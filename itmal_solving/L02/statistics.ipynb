{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS|\n",
    "---------|-----------\n",
    "2018-1218|CEF, initial.                  \n",
    "2019-0131|CEF, spell checked and update. \n",
    "2019-0131|CEF, updated Pearson's r to work with random vars. instead of matrices.\n",
    "2019-0131|CEF, updated mean-var fun and biased/un-biased var estimator update.\n",
    "2019-0204|CEF, minor Pearsons's r update.\n",
    "2019-0204|CEF, changed headline.\n",
    "2019-0204|CEF, added bias comment in cross- auto-covariance.\n",
    "2019-0204|CEF, changed eqs for cross- auto-covariance.\n",
    "2019-0205|CEF, spell checked. \n",
    "\n",
    "## Statistics\n",
    "\n",
    "### Mean and Variance\n",
    "\n",
    "The mean and variance (and hence the standard deviation) for a random variable $X$ can for a population of $N$ samples be estimated as\n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, rember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\st[1]{_{\\mbox{\\scriptsize #1}}}\n",
    "    \\newcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\newcommand\\obs[2]{#1_{\\mbox{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\newcommand\\diff[1]{\\mbox{d}#1}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\mbox{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\mbox{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\begin{array}{rl}\n",
    "        E[X] &= \\frac{1}{N} \\sum_{i=1}^N X_i\\\\\n",
    "             &= \\mu_X\\\\\n",
    "        V[X] &= E[(X_i -E[X])(X_i -E[X])] \\\\\n",
    "             &= E[X_i X_i] - E[X]^2\\\\\n",
    "             &= \\left( \\frac{1}{N} \\sum_{i=1}^N X_i X_i \\right) - \\mu_X^2\\\\\n",
    "             &= \\sigma_X^2\\\\\n",
    "        \\sigma & = \\sqrt{V}\n",
    "    \\end{array}\n",
    "$$\n",
    "\n",
    "Notice that the $1/N$ factor most often appears as $1/(N-1)$ for the variance estimation. \n",
    "\n",
    "When using the factor $1/(N-1)$, $\\hat V$ is said to be the best unbiased estimator, when it is $1/N$ it will be biased, both assuming an underlying normal distribution. \n",
    "\n",
    "Be very careful when rewriting the biased equation above to the unbiased variance estimator. Anyway, for large $N$ this factor has less effect.\n",
    "\n",
    "#### Qa Create a mean and variance function for some input data\n",
    "\n",
    "Your python function should be named ```MeanAndVariance()```, it takes a vector as input parameter, and returns TWO parameters, namely mean and variance. \n",
    "\n",
    "Python allows for return of zero, one, or more parameters from a function, without having to place, say two output parameters in a tuple, like in C++ ```return make_pair<float,float>(mean, variance)```.\n",
    "\n",
    "Test it via the ```y``` input and test-vectors below, go for the biased variance estimator at first.\n",
    "\n",
    "Then extend it to handle both biased and un-biased estimators, create your own test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m= 2.5 , diff= 0.0\n",
      "v_biased= 1.25 , diff= 0.0\n",
      "v_unbiased= 1.6666666666666665 , diff= -2.220446049250313e-16\n",
      "1.25 2.5 1.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# TODO: Qa...\n",
    "\n",
    "def MeanAndVariance(x): \n",
    "    # Your impl here\n",
    "    N_biased = x.size\n",
    "    N_unbiased = x.size - 1\n",
    "    mean = 1/x.size*sum(x)    \n",
    "    variance_biased = 1/N_biased*sum((x - mean)**2)\n",
    "    variance_unbiased = 1/N_unbiased*sum((x - mean)**2)    \n",
    "    return mean, variance_biased, variance_unbiased\n",
    "\n",
    "# TEST vectors: mean and variance calc\n",
    "y = np.array([1,2,3,4])\n",
    "m, v_biased, v_unbiased = MeanAndVariance(y)\n",
    "\n",
    "expected_m = 2.5  \n",
    "expected_v_biased = 1.25 # factor 1/n\n",
    "expected_v_unbiased = 1.6666666666666667 # factor 1/(n-1)\n",
    "\n",
    "print(\"m=\",m,\", diff=\", m-expected_m)\n",
    "print(\"v_biased=\",v_biased,\", diff=\", v_biased-expected_v_biased)\n",
    "print(\"v_unbiased=\",v_unbiased,\", diff=\", v_unbiased-expected_v_unbiased)\n",
    "print(v_biased,np.mean(y), np.var(y)) # np.var is biased(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auto- and Cross-covariance in Matrix Notation\n",
    "\n",
    "Now let's goto full matrix notation for the covariance. For a data matrix $\\bX$  \n",
    "\n",
    "$$\n",
    "    \\bX = \\ac{cccc}{\n",
    "        x_1^{(1)} & x_2^{(1)} & \\cdots & x_d^{(1)} \\\\\n",
    "        x_1^{(2)} & x_2^{(2)} & \\cdots & x_d^{(2)}\\\\\n",
    "        \\vdots \\\\\n",
    "        x_1^{(n)} & x_2^{(n)} & \\cdots & x_d^{(n)}\\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "the previous one-dimensional random variable $X$ can now be seen as a $d$-dimensional vector $\\bx\\powni$, that is one of the data rows in the full data matrix\n",
    "\n",
    "$$\n",
    "    X_i \\to \\bx\\powni = \\left[  x_1\\powni~~ x_2\\powni~~ \\cdots~~ x_d\\powni\\right]^T\n",
    "$$\n",
    "\n",
    "The (biased) autoâ€“covariance matrix can be estimated as\n",
    "\n",
    "$$\n",
    "  \\ar{rl}{\n",
    "            E[\\bX] &= \\mu_\\bX \\\\\n",
    "                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\bx\\powni \\\\ \\\\\n",
    "       \\Sigma(\\bX) &= \\mbox{cov}(\\bX,\\bX) \\\\\n",
    "                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\bx\\powni\\bx^{(i)T} - \\mu_\\bX\\mu_\\bX^T \n",
    "   }\n",
    "   \\rem{\n",
    "     \\ar{ll}{\n",
    "        \\Sigma(\\bX) &= \\mbox{cov}(\\bX,\\bX)\\\\\n",
    "          &= E\\left[(\\bX-\\mu_\\bX)(\\bX-\\mu_\\bX)^T \\right]\\\\\n",
    "          &= E\\left[ \\bX\\bX^T \\right] - E[\\bX]E[\\bX]^T\\\\\n",
    "          &= E\\left[ \\bX\\bX^T \\right] - \\mu_\\bX\\mu_\\bX^T\n",
    "      }\n",
    "   }\n",
    "$$\n",
    "\n",
    "with implicit $1/N$ factor here (inside the first $E[\\cdot]$), so the definition is a biased covariance. You may opt to use the factor $1/(N-1)$ instead.\n",
    "\n",
    "For yet another data matrix $\\bZ $ the (biased) cross-covariance matrix is given by\n",
    "\n",
    "$$\n",
    "  \\ar{ll}{\n",
    "       \\Sigma(\\bX,\\bZ) &= \\mbox{cov}(\\bX,\\bZ)\\\\\n",
    "            &= \\frac{1}{n} \\sum_{i=1}^{n} \\bx\\powni\\bz^{(i)T} - \\mu_\\bX\\mu_\\bZ^T \n",
    "        \\rem{\n",
    "            &= E\\left[(\\bX-\\mu_\\bX)(\\bZ-\\mu_\\bZ)^T \\right]\\\\\n",
    "            &= E\\left[ \\bX\\bZ^T \\right] - E[\\bX]E[\\bZ]^T\\\\\n",
    "            &= E\\left[ \\bX\\bZ^T \\right] - \\mu_\\bX\\mu_\\bZ^T\n",
    "        }\n",
    "   }\n",
    "$$\n",
    "(NOTE: equation to-be confirmed)\n",
    "\n",
    "#### Qb Create a function that generates the auto-covariance matrix $\\Sigma(\\bX)$.\n",
    "\n",
    "Create the function from scratch or find some suitable implementation on the net. \n",
    "\n",
    "Direct use of `numpy.cov` (or other libs) not permitted, though it could serve as a test-stub.  Be very careful regarding the normalization factor when comparing with numpy's or matlabs covar functions. If you are going to use `numpy.cov` for cross-testing, be sure to read up on its `rowvar` parameter!  \n",
    "\n",
    "Make some suitable test data, perhaps share a test-stub with another ITMAL group.\n",
    "\n",
    "Explain the elements in the $\\Sigma$ matrix, what are the diagonal elements $\\Sigma_{ii}$, and what does off-diagonal elements, $\\Sigma_{ij}$ for $i \\neq j$ represent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.17583333, 0.15833333],\n",
       "       [0.15833333, 0.25      ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Covariance\n",
    "def cov(x, y):\n",
    "    xbar, ybar = x.mean(), y.mean()\n",
    "    return np.sum((x - xbar)*(y - ybar))/(len(x) - 1)\n",
    "\n",
    "# Covariance matrix\n",
    "def cov_mat(X):\n",
    "    return np.array([[cov(X[0], X[0]), cov(X[0], X[1])], \\\n",
    "                     [cov(X[1], X[0]), cov(X[1], X[1])]])\n",
    "\n",
    "# Calculate covariance matrix \n",
    "cov_mat(X.T) # (or with np.cov(X.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cov= [[ 0.881875  0.11875   0.5725  ]\n",
      " [ 0.11875   0.1875   -0.225   ]\n",
      " [ 0.5725   -0.225     2.805   ]] , diff= [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qb...\n",
    "import numpy as np\n",
    "\n",
    "def AutoCovariance(X):\n",
    "    X_transposed = np.transpose(X)\n",
    "    size = X.shape[0]\n",
    "    trans_mean = np.mean(X_transposed, axis = 1)\n",
    "    x_mean = np.mean(X, axis = 0)\n",
    "    sumOfVectors = 0\n",
    "    # Iterate through and pull out each vector separately\n",
    "    for i in range(0, size):\n",
    "        # Pulls out vectors: rows in transposed X and columns in X \n",
    "        trans_vec = X_transposed[:,i]\n",
    "        x_vec = X[i,:]\n",
    "        \n",
    "        # Calculates the difference to the mean values\n",
    "        trans_vec_diff = trans_vec - trans_mean\n",
    "        x_vec_deff = x_vec - x_mean\n",
    "        \n",
    "        # Adds the product of the vectors to the sumOfVectors variable\n",
    "        sumOfVectors += np.outer(trans_vec_diff, x_vec_deff)\n",
    "    \n",
    "    return 1/size*(sumOfVectors)\n",
    "    \n",
    "    #X_mean = np.mean(X, axis=0) \n",
    "    #transposed_mean = np.mean(X_transposed, axis = 0)\n",
    "    #print((1/size*np.dot(X_transposed,X) - trans_mean*X_mean))\n",
    "    \n",
    "      \n",
    "# 2 observations, 3 variables. Observed values are held in the 3 columns of the array X   \n",
    "X = np.array([[0.1,2, 0.2], [0.2,1, 0.1], [0.2,1, 4], [-2,1, 0.1]])\n",
    "X_transposed = X.transpose\n",
    "\n",
    "autocov = AutoCovariance(X)\n",
    "\n",
    "# Covariance is a 3x3 array of values. Diagonal elements are the variances in the variables x_i\n",
    "cov = np.cov(X, rowvar=False, bias=True)\n",
    "print()\n",
    "print(\"cov=\",cov,\", diff=\", autocov-cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson's r\n",
    "\n",
    "Now, let us step back again from matrix-vectors to the random variables, $X$ and $Y$. The standard correlation, or Pearson's r, is given by the 'normalized' cross-correlation between the random variables, $X$ and $Y$\n",
    "\n",
    "$$\n",
    "    \\mbox{Pearson's r} = \\rho(X,Y) = \\frac{cov(X,Y)}{\\sigma(X)\\sigma(Y)}\n",
    "$$\n",
    "\n",
    "The (dimensionless) Pearson's r is used in [HOML] and is somewhat similar to the (also dimensionless) $R^2$ score we used last time. \n",
    "\n",
    "Let's try to implement the Pearsons's r now...\n",
    "\n",
    "#### Qc Create a Pearson's r function\n",
    "\n",
    "Again, create some test data, and implement your own function or get inspiration by searching the net for code.\n",
    "\n",
    "You need a full implementation, again not use of libs; so direct use of `numpy.corrcoef` not permitted. But it will serve as a good test function...\n",
    "\n",
    "Write a short description of Pearson's r, giving is range (possible min/max values) and a description of what they mean, and classify it as being a loss/scoring function. \n",
    "\n",
    "Hint: look at these links for the Pearson's r...you can find both algorithm details\n",
    "as well as test data here\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html\n",
    "http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "\n",
    "\n",
    "OPTIONAL: try to re-create some of the data for the plots in figure 2-14 [HOML], and run your Pearson's r function on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
