{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS|\n",
    "---------|------------------------------------------------\n",
    "2018-1218|CEF, initial. \n",
    "2018-1302|CEF, major update.\n",
    "2018-1402|CEF, spell checked.\n",
    "2018-1802|CEF, added class hierarchy figure.\n",
    "\n",
    "## Linear Regression II\n",
    "\n",
    "The goal of the linear regression was to find the argument $\\mathbf w$ that minimizes the objective function, $J$\n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\st[1]{_{\\mbox{\\scriptsize #1}}}\n",
    "    \\newcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\newcommand\\obs[2]{#1_{\\mbox{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\newcommand\\diff[1]{\\mbox{d}#1}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\mbox{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\mbox{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\newcommand\\pfrac[2]{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\newcommand\\dfrac[2]{\\frac{\\mbox{d}~#1}{\\mbox{d}#2}}\n",
    "\\ar{rl}{ \n",
    "    \\bw^* &= \\mbox{argmin}_\\bw~J\\\\\n",
    "          &= \\mbox{argmin}_\\bw\\frac{1}{2} ||\\bX \\bw - \\by||_2^2\n",
    "}\n",
    "$$\n",
    "\n",
    "for some input data, and using the $\\norm{2}^2$ or MSE internally in the loss function.\n",
    "\n",
    "To solve this equation in closed form (directly, without any numerical approximation), we found the optimal solution to be of the rather elegant least-square solution\n",
    "\n",
    "$$\n",
    "  \\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1} \\bX^\\top \\by\n",
    "$$\n",
    "\n",
    "Now, we want to build a python linear regression class that encapsulates this elegant closed-form solution. \n",
    "\n",
    "#### Qa Complete the estimator class `MyRegressor` \n",
    "\n",
    "This one will be based on linear regression closed-form solution from the previous notebook (linear_regression_1.ipynb). \n",
    "\n",
    "Use your knowledge of how to create Python classes and the fit-predict Scikit-learn interface. The class hierarchy will look like\n",
    "\n",
    "<img src=\"Figs/class_regression.png\" style=\"width:210px\">\n",
    "\n",
    "Finalize or complete the `fit()`, `predict()` and `mse()` score functions in the `MyRegressor` class; there is already a good structure for it in the code below.\n",
    "\n",
    "Also, notice the implementation of the `score()` function in the class, that is similar to  `sklearn.linear_model.LinearRegression`'s score function, i.e. a $R^2$ score we saw in an earlier lesson.\n",
    "\n",
    "Use the test stub below, that creates some simple test data, similar to [HOML] p.108/110, and runs a fit-predict on your brand new linear regression closed-form estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, with some help\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import r2_score\n",
    "from libitmal import utils as itmalutils\n",
    "\n",
    "class MyRegressor(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.__w = None\n",
    "        \n",
    "    def weights(self):\n",
    "        return self.__w\n",
    "    \n",
    "    def __addbiasterm(self, X: np.array):\n",
    "        assert X.ndim==2\n",
    "        assert X.shape[0]>0, 'empty X array'\n",
    "        assert X.shape[1]>0, 'empty X array'\n",
    "        X_b = np.c_[np.ones((X.shape[0],1)), X] # Add x0=1 to each instance\n",
    "        return X_b\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array):\n",
    "        # NOTE: really to tight restrictions on input X and y types:\n",
    "        assert str(type(X))==\"<class 'numpy.ndarray'>\"\n",
    "        assert str(type(y))==\"<class 'numpy.ndarray'>\"\n",
    "        \n",
    "        assert X.ndim==2, f'expected X array of ndim=2, got ndim={X.ndim}'\n",
    "        assert y.ndim==1, f'expected y array of ndim=1, got ndim={y.ndim}'\n",
    "        assert X.shape[0]==y.shape[0], 'expected X.shape[0] to be equal to y.shape[0], got X.shape[0]={X.shape[0]}, y.shape[0]={y.shape[0]}'\n",
    "                \n",
    "        # y=Xw, where X is a matrix with full column rank, the least squares solution,\n",
    "        #\n",
    "        #   w^=argmin∥Xw−y∥2\n",
    "        #\n",
    "        # is given by\n",
    "        #\n",
    "        #   w^=(X^TX)^−1 X^T y \n",
    "        \n",
    "        # HOML, p.111\n",
    "        assert False, 'TODO: implement it: add the least-square calc of w'\n",
    "        \n",
    "        #self.__w = ...\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.array):\n",
    "        #if (not self.__w):\n",
    "        #    raise RuntimeError('You must train model before predicting data!')\n",
    "        \n",
    "        X_b = self.__addbiasterm(X)\n",
    "        \n",
    "        assert X_b.ndim==2\n",
    "        assert X_b.shape[1]==self.__w.shape[0]\n",
    "        \n",
    "        assert False, 'TODO: implement it: add the predictor code'\n",
    "        \n",
    "        #p = ...\n",
    "        \n",
    "        assert p.ndim==1\n",
    "        assert p.shape[0]==X.shape[0]\n",
    "        \n",
    "        return p\n",
    "\n",
    "    def score(self, X: np.array, y: np.array):\n",
    "        # Returns the coefficient of determination R^2 of the prediction.\n",
    "        # The coefficient R^2 is defined as (1 - u/v), where u is the residual \n",
    "        # sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum\n",
    "        # of squares ((y_true - y_true.mean()) ** 2).sum().  \n",
    "        \n",
    "        p=self.predict(X)\n",
    "                \n",
    "        assert p.ndim==1\n",
    "        assert y.ndim==1\n",
    "        assert p.shape==y.shape\n",
    "    \n",
    "        return r2_score(y, p) \n",
    "    \n",
    "    def mse(self, X: np.array, y: np.array):\n",
    "        p = self.predict(X)\n",
    "        \n",
    "        assert p.shape==y.shape        \n",
    "        diff=y-p        \n",
    "        assert diff.ndim==1\n",
    "        \n",
    "        n=y.shape[0]\n",
    "        assert n>0\n",
    "        \n",
    "        # could use np.dot() but lets be verbose\n",
    "        J=0.0 \n",
    "        assert False, 'TODO: implement it: add the J sum functionality via MSE' \n",
    "           \n",
    "        #...\n",
    "            \n",
    "        itmalutils.CheckFloat(J)\n",
    "        if J<0:\n",
    "            raise RangeError(f'expeted J to be >= 0, got J={J}')\n",
    "        \n",
    "        return J\n",
    "\n",
    "    def __str__ (self):\n",
    "        return f'MyRegressor: _w={self.__w}'\n",
    "    \n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SECTION for Qa...or use your own tests!\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from libitmal import utils as itmalutils\n",
    "\n",
    "def PrintResults(msg, f, X: np.array, y: np.array, p: np.array):\n",
    "    print(msg)\n",
    "    \n",
    "    reg_score=reg.score(X,y)\n",
    "    mse=mean_squared_error(p,y)\n",
    "    try:\n",
    "        reg_mse=reg.mse(X,y)\n",
    "    except AttributeError:\n",
    "        reg_mse = mse # fall back\n",
    "    \n",
    "    print(f'  f={f}')\n",
    "    #print(f'  y={y}')\n",
    "    #print(f'  p={p}')\n",
    "    #print(f'  y-p={y-p}')\n",
    "    print(f'  reg_score={reg_score}')\n",
    "    print(f'  reg_mse={reg_mse}')\n",
    "    print(f'  mean_squared_error(y,p)={mse}') \n",
    "    \n",
    "    itmalutils.AssertInRange(reg_mse,mse)\n",
    "    return reg_mse, reg_score\n",
    "\n",
    "def DoPlot(X, y):\n",
    "    % matplotlib inline\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.xlabel('$x_1$', fontsize=18)\n",
    "    plt.ylabel('$y$', rotation=0, fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.show()\n",
    "\n",
    "itmalutils.ResetRandom()\n",
    "\n",
    "M=200\n",
    "N=20\n",
    "\n",
    "print(f'M={10}, N={N}')\n",
    "\n",
    "X=2 * np.random.rand(M,N)\n",
    "y=4 + 3*X + np.random.randn(M,1)\n",
    "y=y[:,0] # well, could do better here!\n",
    "\n",
    "DoPlot(X, y)\n",
    "  \n",
    "print('Test data for classifier:')\n",
    "print(f'  X.shape={X.shape}')\n",
    "print(f'  y.shape={y.shape}\\n')\n",
    "\n",
    "reg = MyRegressor()\n",
    "\n",
    "f=reg.fit(X, y)\n",
    "p=reg.predict(X)\n",
    "\n",
    "assert p.shape==y.shape\n",
    "\n",
    "reg_mse, reg_score = PrintResults('Result for MyRegressor...', f, X, y, p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb Compare your linear regressor with the one from Scikit-learn\n",
    "\n",
    "Fit and predict the same data to the `sklearn.linear_model.LinearRegression` model.\n",
    "\n",
    "Check that the `LinearRegression.score` and `sklearn.metrics.mean_squared_error` yields the same value as found in your estimators public functions `score()` and `mse()`. \n",
    "\n",
    "Notice that Scikit-leans `LinearRegression` does not have a built-in `mse()` function. In the test-stubs I just use the  `sklearn.metrics.mean_squared_error` function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc Compare model parameters \n",
    "\n",
    "Check the model parameter. Is the weight vectors for the two models similar?\n",
    "\n",
    "Extend the dimensionality in the test stub, say `M=200, N=20`, still the same `mse()`, `score()` and model weight vectors for `MyRegressor` and `LinearRegression`?\n",
    "\n",
    "Notice the _niftyness_ of the `itmalutils.AssertInRange()` function...check it out in the libitmal library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qd  Closed-form vs. numerical solutions,\n",
    "\n",
    "When using the closed-form solution, what happens if:\n",
    "* the dataset is very large and taking the inverse of a large matrix?\n",
    "* the cost function is non-convex, with multiple minima?\n",
    "* the underlying model is non-linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd...text only..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
